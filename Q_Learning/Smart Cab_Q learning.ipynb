{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "#for text processing\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env #Taxi-v2 is not available now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Origing, Destination, and Time of Pickup from the sms data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fetch_pickup_drop(line):\n",
    "    city = pd.read_csv(\"./city.csv\", header = None)\n",
    "    city_list = list(city[0].values)\n",
    "    r1 = re.findall(r\"[\\d+]+ AM\", line)\n",
    "    if len(r1):\n",
    "        time = r1[0].strip()\n",
    "    else:\n",
    "        r1 = re.findall(r\"[\\d+]+ PM\", line)\n",
    "        time = r1[0].strip()\n",
    "        \n",
    "    time_of_pickup = time        \n",
    "    ##Fetching Origin and Destination\n",
    "    candidate_list = []\n",
    "    for city in city_list:\n",
    "        if city in line:\n",
    "            candidate_list.append(city)\n",
    "\n",
    "    if len(candidate_list) == 2:\n",
    "        flag = 0\n",
    "        for city in candidate_list:\n",
    "            pattern1 = 'to '+city\n",
    "            pattern2 = 'for '+city\n",
    "            r1 = re.findall(pattern1, line)\n",
    "            r2 = re.findall(pattern2, line)\n",
    "            if len(r1) or len(r2):\n",
    "                destination = city\n",
    "                candidate_list.remove(city)\n",
    "                origin = candidate_list[0]\n",
    "                flag =1\n",
    "                break\n",
    "        return[origin, destination, time_of_pickup]\n",
    "\n",
    "        if flag == 0:\n",
    "            print(\"Destination not found for patter1\", pattern1)\n",
    "            print(\"Destination not found for patter2\", pattern1)\n",
    "            print(\"line is\", line)\n",
    "            return [origin, destination, time_of_pickup]\n",
    "\n",
    "    else:\n",
    "        print(\"Does not include both origin and destination\")\n",
    "        origin = 'NA'\n",
    "        destination = 'NA'\n",
    "        return [origin, destination, time_of_pickup]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up the Q-Learning Process\n",
    "Breaking it down into steps, we get\n",
    "\n",
    "Initialize the Q-table by all zeros.\n",
    "\n",
    "Start exploring actions: \n",
    "\n",
    "For each state, select any one among all possible actions for the current state (S).\n",
    "\n",
    "Travel to the next state (S') as a result of that action (a).\n",
    "\n",
    "For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "\n",
    "Update Q-table values using the equation.\n",
    "\n",
    "Set the next state as the current state.\n",
    "\n",
    "If goal state is reached, then end and repeat the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting learned values\n",
    "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
    "\n",
    "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called Ïµ \"epsilon\" to cater to this during training.\n",
    "\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Initialize Q_table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001): #if reduced the range, then agent will stuck in loop not sure what was happening though.\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # exploit\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "\n",
    "np.save(\"./q_table.npy\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained q_table for evaluation\n",
    "\n",
    "q_table = np.load(\"./q_table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loc_dict(city_df):\n",
    "    loc_dict = {}\n",
    "    ## Create dictionary example, loc_dict['dwarka sector 23] = 0\n",
    "    for i in range(len(city_df)):\n",
    "        loc_dict.update({city_df.iloc[i].location:city_df.iloc[i].mapping})\n",
    "        \n",
    "    return loc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pick_up_drop_correction(pick_up, drop, line_num):\n",
    "    orig_df = pd.read_csv(\"./org_df.csv\")\n",
    "    original_origin = orig_df.iloc[line_num]['origin']\n",
    "    original_destination = orig_df.iloc[line_num]['dest']\n",
    "    if original_origin == pick_up and original_destination == drop:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 1000 episodes:\n",
      "Average timesteps per episode: 130.914\n",
      "Average penalties per episode: 0.0\n",
      "Total number of wrong predictions 0\n",
      "Total Reward is 200000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "# 1) We need to take text drom \"sms.txt\" and fetch pickup and drop from it.\n",
    "# 2) Generate the random state from an enviroment and change the pick up and drop as the fetched one\n",
    "# 3) Evaluate you q_table performance on all the texts given in sms.txt.\n",
    "# 4) Have a check if the fetched pickup, drop is not matching with original pickup, drop using orig.csv\n",
    "# 5) If fetched pickup or/and drop does not match with the original, add penality and reward -10\n",
    "# 6) Calculate the Total reward, penalities, Wrong pickup/drop predicted and Average time steps per episode.\n",
    "\n",
    "total_epochs, total_penalties, total_reward, wrong_predictions = 0, 0, 0, 0\n",
    "\n",
    "\n",
    "count = 0\n",
    "time_list = []\n",
    "f = open(\"./sms.txt\", \"r\")\n",
    "num_of_lines = 1000\n",
    "city = pd.read_csv(\"./city.csv\")\n",
    "\n",
    "loc_dict = create_loc_dict(city)\n",
    "line_num = 0\n",
    "for line in f:\n",
    "    pickup, drop, time = fetch_pickup_drop(line)\n",
    "    pickUpIdx  = loc_dict[pickup]\n",
    "    dropIdx = loc_dict[drop]\n",
    "    env.reset()\n",
    "    #print(line_num)\n",
    "    decision = check_pick_up_drop_correction(pickup,drop,line_num)\n",
    "    \n",
    "    if not decision:\n",
    "        total_penalities += 1\n",
    "        reward = -10\n",
    "        total_reward += reward\n",
    "        wrong_predictions += 1\n",
    "    \n",
    "    episodes = 10\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "        \n",
    "        done = False\n",
    "        env.encode(np.random.randint(0,4),np.random.randint(0,4),pickUpIdx,dropIdx)\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "            #print(state,reward,done,\" \",line_num,episodes)\n",
    "            #if reward == 20:\n",
    "             #   print(\"20$ reward\")\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "            epochs += 1\n",
    "    \n",
    "        total_penalties += penalties\n",
    "        total_epochs += epochs\n",
    "        total_reward += reward\n",
    "    \n",
    "    line_num = line_num + 1\n",
    "\n",
    "print(f\"Results after {num_of_lines} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / num_of_lines}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / num_of_lines}\")\n",
    "print(f\"Total number of wrong predictions\", wrong_predictions)\n",
    "print(\"Total Reward is\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
